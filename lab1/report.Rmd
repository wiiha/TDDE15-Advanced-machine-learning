---
title: "TDDE15 - LAB 1"
author: "Wilhelm Hansson"
date: "2020-09-12"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(bnlearn)
library(Rgraphviz)
library(gRain)

source("./helper-functions.R")

data("asia")
```

# Task 1

*Show that multiple runs of the hill-climbing algorithm can return non-equivalent Bayesian network (BN) structures. Explain why this happens.*

```{r 1_1, include=FALSE}
# Testing different random restarts
set.seed(567)
bn.hc.5 <- hc(x = asia, restart = 5)

set.seed(567)
bn.hc.10 <- hc(x = asia, restart = 10)

set.seed(567)
bn.hc.1 <- hc(x = asia, restart = 1)

set.seed(567)
bn.hc.100 <- hc(x = asia, restart = 100)

```

```{r}
plot(bn.hc.10, main="Random restarts: 10")
```

```{r}
plot(bn.hc.5, main="Random restarts: 5")
```

```{r}
plot(bn.hc.1, main="Random restarts: 1")
```

```{r}
plot(bn.hc.100, main="Random restarts: 100")
```

```{r 1_2, include=FALSE}
nodes <- colnames(asia)
init <- empty.graph(nodes)
print(init)

arc.set.1 = matrix(c("A", "S", "X", "D", "E", "X", "X", "T"),
                   ncol = 2, byrow = TRUE,
                   dimnames = list(NULL, c("from", "to")))
print(arc.set.1)

arc.set.2 = matrix(c("B", "D", "X", "D"),
                   ncol = 2, byrow = TRUE,
                   dimnames = list(NULL, c("from", "to")))
print(arc.set.2)

arc.set.3 = matrix(c("S", "A"),
                   ncol = 2, byrow = TRUE,
                   dimnames = list(NULL, c("from", "to")))
print(arc.set.3)


set.seed(234)
bn.hc.arc.0 <- hc(x = asia)

set.seed(234)
arcs(init) = arc.set.1
print(init)
bn.hc.arc.1 <- hc(x = asia, start=init)

set.seed(234)
arcs(init) = arc.set.2
print(init)
bn.hc.arc.2 <- hc(x = asia, start=init)

set.seed(234)
arcs(init) = arc.set.3
print(init)
bn.hc.arc.3 <- hc(x = asia, start=init)
```

```{r}
plot(bn.hc.arc.0, main="arc.set 0")
```

```{r}
plot(bn.hc.arc.1, main="arc.set 1")
```

```{r}
plot(bn.hc.arc.2, main="arc.set 2")
```

```{r}
plot(bn.hc.arc.3, main="arc.set 3")
```

*Finding: If the initial structure just has one or two directed arcs the hc process seem to fallback to the same final graph*

```{r 1_3, include=FALSE}
# Testing different scoring methodes
score.methodes = c("bic","loglik","aic","mbde")

set.seed(173)
bn.hc.s.1 <- hc(x = asia, score = score.methodes[1])

set.seed(173)
bn.hc.s.2 <- hc(x = asia, score = score.methodes[2])

set.seed(173)
bn.hc.s.3 <- hc(x = asia, score = score.methodes[3])

set.seed(173)
bn.hc.s.4 <- hc(x = asia, score = score.methodes[4])

```

```{r}

plot(bn.hc.s.1, main= paste("Scoring method:",score.methodes[1]))

```

```{r}
plot(bn.hc.s.2, main= paste("Scoring method:",score.methodes[2]))

```

```{r}
plot(bn.hc.s.3, main= paste("Scoring method:",score.methodes[3]))

```

```{r}
plot(bn.hc.s.4, main= paste("Scoring method:",score.methodes[4]))

```

*Finding: This method resulted in the larges differences between each final graph.*

**Conclusion:** The chaining of scoring method seem to have largest impact when learning the structure of a BN. However, the number of random restarts also seem to contribute to the final BN. These findings align with the fact that the hill-climbing algorithm starts at a random point in graph space and the tries different graphs and evaluate the score for each of these graphs. By doing multiple restarts more of graph space is explored. Further one can notice that the *log-likelihood* scoring function produced a graph with many edges. A graph with many edges can be interpreted as an overfitted model. *BIC* and *AIC* solve this by implementing a constraint/ punishment to the score. Both methodes can be found in the paper by Koski and  Noble (2012).




# Task 2

*Learn a BN from 80 % of the Asia dataset. Use the BN learned to classify the remaining 20 % of the Asia dataset in two classes: S = yes and S = no. Use exact or approximate inference with the help of the bnlearn and gRain packages, you are not allowed to use functions such as predict. Report the confusion matrix; true/false positives/negatives.*

```{r 2_1, include=FALSE}
set.seed(2020)

# Splitting data 80/20 train/test
n=nrow(asia)
id=sample(1:n,floor(n*0.8))
train=asia[id,]
test=asia[-id,]

# Learning the structure of the BN.
bn.structure <- hc(x = train, restart = 15, score = "bic")
# Learning the parameters.
bn.fitted <- bn.fit(x = bn.structure, data = train, method = "bayes")

# Setting the true structure of the BN
bn.true.structure = model2network("[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]")
# Learning the parameters using the true structure.
bn.true.fitted <- bn.fit(x = bn.true.structure, data = train, method = "bayes")

# Converting to grain obj. to use in compile.grain.
bn.fitted.grain <- as.grain(bn.fitted)
bn.true.fitted.grain <- as.grain(bn.true.fitted)


# Now creating junction trees using compile.grain.
# The trees will be queried to make predictions for S.
junction.tree.own <- compile(bn.fitted.grain)
junction.tree.true <- compile(bn.true.fitted.grain)

# Making predictions using helper-function (separate file)
prediction.own <- predictfunction(junction.tree.own,test,"S")
prediction.true <- predictfunction(junction.tree.true,test,"S")

# Comparing predictions with true values
conf.own <- make.confusion.matrix(test[,"S"],prediction.own)
conf.true <- make.confusion.matrix(test[,"S"],prediction.true)

```

Below the learned sturcture and the true structure is presented for comparision.

```{r}
plot(bn.structure, main="Learned structure")
```


```{r}
plot(bn.true.structure, main="True structure")
```


```{r}
print("Confusion matrix using learned structure")
print(conf.own)
print("Confusion matrix using true structure")
print(conf.true)
```

**Finding:** As one can observed, the two confusion matrices are the same even thou the structure used for prediction was not.


# Task 3

*In the previous exercise, S was classified using observations for all the rest of the variables. S should be classified by using only observations for the so-called Markov blanket of S, i.e. its parents plus its children plus the parents of its children minus S itself.*

```{r 3_1, include=FALSE}
# Making predictions using helper-function (separate file)
prediction.own <- predictfunction(junction.tree.own,test,"S", bn.for.markowblanket = bn.structure)
prediction.true <- predictfunction(junction.tree.true,test,"S", bn.for.markowblanket = bn.true.structure)

# Comparing predictions with true values
conf.own <- make.confusion.matrix(test[,"S"],prediction.own)
conf.true <- make.confusion.matrix(test[,"S"],prediction.true)

```

```{r}
print("Markow blanket for learned structure")
print(mb(bn.structure,"S"))
print("Markow blanket for true structure")
print(mb(bn.true.structure,"S"))
```


```{r}
print("Confusion matrix using learned structure")
print(conf.own)
print("Confusion matrix using true structure")
print(conf.true)
```

**Finding:** As one can observed, the two confusion matrices are the same when the Markow blanket for eact sturcture was used.

#Task 4

*Repeat the exercise (2) using a naive Bayes classifier, i.e. the predictive variables are independent given the class variable. Model the naive Bayes classifier as a BN. You have to create the BN by hand.*

```{r 4, include=FALSE}

# Learning the structure of the BN.
bn.structure <- model2network("[S][A|S][T|S][L|S][B|S][E|S][X|S][D|S]")

# Learning the parameters.
bn.fitted <- bn.fit(x = bn.structure, data = train, method = "mle")

# Converting to grain obj. to use in compile.grain.
bn.fitted.grain <- as.grain(bn.fitted)

# Now creating junction trees using compile.grain.
# The trees will be queried to make predictions for S.
junction.tree.own <- compile(bn.fitted.grain)

# Making predictions using helper-function (separate file)
prediction.own <- predictfunction(junction.tree.own,test,"S")

# Comparing predictions with true values
conf.own <- make.confusion.matrix(test[,"S"],prediction.own)

```

In the figure below is the Naive Bayes structure for the BN.

```{r}
# plot(bn.structure)
graphviz.plot(bn.structure, layout = "neato")
```

Below is the confusion matrix for the Naive Bayes classifier presented.
```{r}
print(conf.own)
print(prop.table(conf.own))
```

# Task 5

*Explain why you obtain the same or different results in the exercises (2-4).*

\newpage

# Appendix for code

```{r, code=readLines("lab1-full.R"), echo=TRUE, eval=FALSE}
```
